cluster:
  mkdir -p ./logs/{rule} &&
  sbatch
    --partition={resources.partition}
    --time={resources.time}
    --job-name={rule}.{wildcards}
    --output=./logs/{rule}/%j.out
    --error=./logs/{rule}/%j.err
    --parsable
    --mem={resources.mem}
    --gpus-per-task={resources.gpus}
    --nodes={resources.nodes}
    --ntasks-per-node={resources.tasks}
    --cpus-per-task={resources.threads}
default-resources:
  - partition=normal,owners
  - time="00:10:00"
  - mem=4000
  - nodes=1
  - threads=1
  - tasks=1
  - gpus=0
latency-wait: 120
# restart-times: 3
jobs: 50
keep-going: True
rerun-incomplete: True
printshellcmds: True
scheduler: greedy
use-conda: True
# Singularity args (binds to oak)
#use-singularity: True
#singularity-args: "-B /oak:/oak"
cluster-status: "config/slurm_sherlock/status-sacct.sh"
max-status-checks-per-second: 10
cluster-cancel: scancel
